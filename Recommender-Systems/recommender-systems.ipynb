{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "\n",
    "## DO NOT 'RUN ALL' the cells, some of them take a really long time, read the content before running anything\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "NOTE : resources folder could not be included on github due to size. It also stores cached NMF results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data info\n",
      "       user_from_id    user_to_id\n",
      "count  76392.000000  76392.000000\n",
      "mean    1135.738297   1817.058174\n",
      "std      818.555235   1059.191942\n",
      "min        1.000000      0.000000\n",
      "25%      443.000000    887.000000\n",
      "50%      988.000000   1807.000000\n",
      "75%     1707.000000   2733.250000\n",
      "max     3716.000000   3624.000000\n",
      "\n",
      "Test data info\n",
      "       user_from_id    user_to_id\n",
      "count  16203.000000  16203.000000\n",
      "mean    1389.043264   1777.779053\n",
      "std      952.172409   1053.127740\n",
      "min        0.000000      0.000000\n",
      "25%      583.000000    835.000000\n",
      "50%     1219.000000   1836.000000\n",
      "75%     2087.500000   2676.000000\n",
      "max     3719.000000   3624.000000\n"
     ]
    }
   ],
   "source": [
    "train_file_path = 'resources/lab2_train.csv'\n",
    "test_file_path = 'resources/lab2_test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_file_path, delimiter=',')\n",
    "\n",
    "train_bool = train_data.copy()\n",
    "train_bool['is_match'] = train_bool['is_match'].astype('bool')\n",
    "\n",
    "test_data = pd.read_csv(test_file_path, delimiter=',')\n",
    "\n",
    "print(\"Train data info\")\n",
    "print(train_data.describe())\n",
    "print(\"\\nTest data info\")\n",
    "print(test_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by examing the distribution of likes and matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Like: 16.54230809508849%\n",
      "Match: 0.5432506021572939%  Nulls: 1.0668656403811918%\n",
      "\n",
      "Train data bool (like stays the same)\n",
      "Match percentage: 1.6101162425384858%\n"
     ]
    }
   ],
   "source": [
    "train_n = train_data.shape[0]\n",
    "train_likes = train_data[train_data['is_like'] == True].shape[0]\n",
    "train_matches = train_data[train_data['is_match'] == True].shape[0]\n",
    "train_nulls = train_data[train_data['is_match'].isnull() == True].shape[0]\n",
    "print(\"Train data\")\n",
    "print(f'Like: {train_likes / train_n * 100}%')\n",
    "print(f\"Match: {train_matches / train_n * 100}%  Nulls: {train_nulls / train_n * 100}%\")\n",
    "\n",
    "\n",
    "bool_n = train_bool.shape[0]\n",
    "bool_matches = train_bool[train_bool['is_match'] == True].shape[0]\n",
    "print(\"\\nTrain data bool (like stays the same)\")\n",
    "print(f\"Match percentage: {bool_matches / bool_n * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that is_like and is_match are both sparse, with only 0.5% of entries being a match.\n",
    "\n",
    "Null values in is_match probably represent someone who has been liked, but not \"responded\" with a like or rejection of their own. These values can be removed while training.\n",
    "\n",
    "We move on to some vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_given = df['user_from_id'].value_counts()\n",
    "likes_received = df['user_to_id'].value_counts()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(likes_given, kde=False, color='blue', label='Likes Given')\n",
    "sns.histplot(likes_received, kde=False, color='orange', label='Likes Received')\n",
    "plt.title('Distribution of Likes Given and Received')\n",
    "plt.xlabel('likes given / received')\n",
    "plt.ylabel('nr of users')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(likes_given[likes_given > 75], kde=False, color='blue', label='Likes Given')\n",
    "sns.histplot(likes_received[likes_received > 75], kde=False, color='orange', label='Likes Received')\n",
    "plt.title('Distribution of likes Given and Received - top preformers')\n",
    "plt.xlabel('likes given / received')\n",
    "plt.ylabel('nr of users')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Calculate the number of likes received by each user\n",
    "likes_received = train_data[train_data['is_like']].groupby('user_to_id').size()\n",
    "\n",
    "# Determine the top 10% of users based on likes received\n",
    "top_percentage = 0.1\n",
    "top_users_count = int(len(likes_received) * top_percentage)\n",
    "top_user_ids = likes_received.nlargest(top_users_count).index\n",
    "\n",
    "# Add all user nodes to the graph\n",
    "G.add_nodes_from(train_data['user_from_id'])\n",
    "G.add_nodes_from(train_data['user_to_id'])\n",
    "\n",
    "# Add edges from top users\n",
    "for _, row in train_data.iterrows():\n",
    "    if row['user_to_id'] in top_user_ids or row['user_from_id'] in top_user_ids:\n",
    "        G.add_edge(row['user_from_id'], row['user_to_id'], is_like=row['is_like'], is_match=row['is_match'])\n",
    "\n",
    "# Define node colors based on whether the user is a top user or not\n",
    "node_color = ['yellow' if node in top_user_ids else 'skyblue' for node in G]\n",
    "\n",
    "# Define edge colors based on whether it's a like or a match\n",
    "edge_color = ['green' if data['is_match'] else 'gray' for _, _, data in G.edges(data=True)]\n",
    "\n",
    "# Position nodes using a spring layout\n",
    "pos = nx.spring_layout(G, k=0.2, iterations=20)\n",
    "\n",
    "# Draw the nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_color, alpha=0.7)\n",
    "\n",
    "# Draw the edges\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_color, alpha=0.5)\n",
    "\n",
    "# Set plot title and show graph\n",
    "plt.title(\"User Interactions: Likes and Matches\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top users likes\n",
    "\n",
    "# init df\n",
    "df = train_data.copy()\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# find top 10% of users (most received likes)\n",
    "likes_received = df[df['is_like']].groupby('user_to_id').size()\n",
    "top_users = likes_received.sort_values(ascending=False)\n",
    "top_percentage = 0.1\n",
    "top_users_count = int(len(top_users) * top_percentage)\n",
    "\n",
    "top_user_ids = top_users.head(top_users_count).index\n",
    "top_users_df = df[df['user_to_id'].isin(top_user_ids)]\n",
    "\n",
    "\n",
    "# Add nodes\n",
    "nodes = pd.concat([df['user_from_id'], df['user_to_id']])\n",
    "G.add_nodes_from(nodes.unique())\n",
    "\n",
    "# add edges for top users\n",
    "for _, row in df.iterrows():\n",
    "    if row['user_from_id'] in top_user_ids:\n",
    "        G.add_edge(row['user_from_id'], row['user_to_id'], is_like=row['is_like'], is_match=row['is_match'])\n",
    "\n",
    "# Define node colors based on whether the user is a top user or not\n",
    "node_color = ['yellow' if node in top_user_ids else 'skyblue' for node in G]\n",
    "\n",
    "# Define edge colors based on whether it's a like or a match\n",
    "edge_color = ['green' if data['is_match'] else 'gray' for _, _, data in G.edges(data=True)]\n",
    "\n",
    "# set figure and pos\n",
    "plt.figure(figsize=(20, 11))\n",
    "pos = nx.spring_layout(G, k=0.2, iterations=20)\n",
    "\n",
    "# Draw nodes\n",
    "node_degree = dict(G.degree)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=[(1 + v) for v in node_degree.values()], node_color=node_color, alpha=0.7)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_color, alpha=0.5)\n",
    "\n",
    "# Set plot title and show graph\n",
    "plt.title(\"User Interactions: Likes and Matches\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges with weights for likes\n",
    "for _, row in train_data.iterrows():\n",
    "    G.add_node(row['user_from_id'])\n",
    "    G.add_node(row['user_to_id'])\n",
    "    weight = 1 if row['is_like'] else 0\n",
    "    match_weight = 3 if row['is_match'] else 0\n",
    "    G.add_edge(row['user_from_id'], row['user_to_id'], weight=weight, match_weight=match_weight)\n",
    "\n",
    "# Use a force-directed layout to spread nodes out\n",
    "plt.figure(figsize=(20, 11))\n",
    "pos = nx.spring_layout(G, k=0.3, iterations=20)\n",
    "\n",
    "# Draw nodes\n",
    "node_degree = dict(G.degree)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=[v * 2 for v in node_degree.values()], node_color='skyblue', alpha=0.7)\n",
    "\n",
    "# Draw likes edges\n",
    "likes_edges = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] == 1]\n",
    "nx.draw_networkx_edges(G, pos, edgelist=likes_edges, edge_color='gray', alpha=0.05)\n",
    "\n",
    "# # Draw matches edges\n",
    "# matches_edges = [(u, v) for (u, v, d) in G.edges(data=True) if d['match_weight'] == 3]\n",
    "# nx.draw_networkx_edges(G, pos, edgelist=matches_edges, edge_color='green', alpha=0.5)\n",
    "\n",
    "# Set plot title and show graph\n",
    "plt.title(\"User Interactions: Likes and Matches\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnm_train = train_data.copy()\n",
    "nnm_test = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_loss(V, W, H):\n",
    "    # Create mask \n",
    "    mask = V != 0\n",
    "\n",
    "    # find error\n",
    "    reconstruction = np.dot(W, H)\n",
    "    error = V - reconstruction\n",
    "\n",
    "    # Apply mask and return frobenius norm.\n",
    "    masked_error = np.where(mask, error, 0)\n",
    "    return np.linalg.norm(masked_error, 'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf(X: pd.DataFrame, n_components: int, max_iter: int=1000, tol: float=1e-3, debug: bool=False):\n",
    "  \"\"\"\n",
    "  Decomposes the original sparse matrix X into two matrices W and H. \n",
    "  \"\"\"\n",
    "  # Initialize W and H with random non-negative values\n",
    "  W = np.random.rand(X.shape[0], n_components)\n",
    "  H = np.random.rand(n_components, X.shape[1])\n",
    "\n",
    "  # Set constants\n",
    "  epsilon = 1e-9;\n",
    "  V = X.to_numpy() if isinstance(X, pd.DataFrame) else X;\n",
    "\n",
    "  # Construct intial error\n",
    "  error = nmf_loss(V, W, H);\n",
    "  new_error = 0;\n",
    "\n",
    "  iterations = 0;\n",
    "  while (error - new_error) > tol and iterations <= max_iter:\n",
    "    if (debug and iterations%10 == 0):\n",
    "      print(f\"{iterations}/{max_iter}\")\n",
    "    W_update = np.dot(V, H.T) / (np.dot(np.dot(W, H), H.T) + epsilon)\n",
    "    W *= W_update \n",
    "\n",
    "    H_update = np.dot(W.T, V) / (np.dot(np.dot(W.T, W), H) + epsilon)\n",
    "    H *= H_update\n",
    "\n",
    "    new_error = nmf_loss(V, W, H)\n",
    "    iterations+=1;\n",
    "\n",
    "\n",
    "  return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_optimized(X: pd.DataFrame, n_components: int, max_iter: int = 1000, tol: float = 1e-3, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Decomposes the original sparse matrix X into two matrices W and H.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set constants\n",
    "    epsilon = 1e-9\n",
    "    V = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "    # Initialize W and H with random non-negative values, scaled to max of X\n",
    "    max = V.flatten().max();\n",
    "    W = np.random.rand(X.shape[0], n_components) * max;\n",
    "    H = np.random.rand(n_components, X.shape[1]) * max;\n",
    "\n",
    "    # Construct initial error\n",
    "    error = nmf_loss(V, W, H)\n",
    "    new_error = 0\n",
    "\n",
    "    iterations = 0\n",
    "    while iterations < max_iter:\n",
    "        if debug and iterations % 10 == 0:\n",
    "            print(f\"{iterations}/{max_iter}\")\n",
    "\n",
    "        # Update W\n",
    "        W_update = np.dot(V, H.T) / (np.dot(np.dot(W, H), H.T) + epsilon)\n",
    "        W *= W_update\n",
    "        new_error = nmf_loss(V, W, H)\n",
    "        \n",
    "        # Check for convergence after updating W\n",
    "        if abs(error - new_error) < tol:\n",
    "            break\n",
    "        error = new_error\n",
    "\n",
    "        # Update H\n",
    "        H_update = np.dot(W.T, V) / (np.dot(np.dot(W.T, W), H) + epsilon)\n",
    "        H *= H_update\n",
    "        new_error = nmf_loss(V, W, H)\n",
    "\n",
    "        # Check for convergence after updating H\n",
    "        if abs(error - new_error) < tol:\n",
    "            break\n",
    "        error = new_error\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "    return W, H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_validate(prediction_matrix, validation_set, threshhold):\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "\n",
    "    for _, row in validation_set.iterrows():\n",
    "        pred_value = prediction_matrix.iloc[row['user_from_id'], row['user_to_id']]\n",
    "        pred = 2 if pred_value > threshhold else 1\n",
    "\n",
    "        if(row['is_like'] and pred == 2):\n",
    "            TP+=1\n",
    "        if(row['is_like'] and pred == 1):\n",
    "            FN+=1\n",
    "        if(not row['is_like'] and pred == 2):\n",
    "            FP+=1\n",
    "        if(not row['is_like'] and pred == 1):\n",
    "            TN+=1\n",
    "\n",
    "    \n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return {\"FP\": FP, \"FN\": FN, \"TP\": TP, \"TN\": TN, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking if we can remove any outliers, and what an outlier represents in the context of the dating app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper:0.7142857142857143  lower:0.0\n",
      "outliers removed 57 out 76392\n",
      "        like_count  total_count  rejection_count   like_ratio\n",
      "count  3040.000000  3040.000000      3040.000000  3040.000000\n",
      "mean      4.156908    25.128947        20.972039     0.154255\n",
      "std       7.176296    28.938301        24.850343     0.208570\n",
      "min       0.000000     1.000000         0.000000     0.000000\n",
      "25%       0.000000     6.000000         5.000000     0.000000\n",
      "50%       1.000000    16.000000        13.000000     0.044281\n",
      "75%       5.000000    33.250000        27.000000     0.270270\n",
      "max      57.000000   362.000000       350.000000     1.000000\n",
      "            like_count  total_count  rejection_count  like_ratio\n",
      "user_to_id                                                      \n",
      "3130                 3            3                0    1.000000\n",
      "645                  1            1                0    1.000000\n",
      "2712                 1            1                0    1.000000\n",
      "1973                 2            2                0    1.000000\n",
      "1522                 2            2                0    1.000000\n",
      "515                  2            2                0    1.000000\n",
      "808                  5            5                0    1.000000\n",
      "524                  2            2                0    1.000000\n",
      "3559                12           13                1    0.923077\n",
      "2458                 1            1                0    1.000000\n"
     ]
    }
   ],
   "source": [
    "# create user profiles with like stats\n",
    "user_profiles = nnm_train.groupby('user_to_id')['is_like'].agg(like_count='sum', total_count='count')\n",
    "user_profiles['like_count'] = user_profiles['like_count'].astype(int)  \n",
    "user_profiles['rejection_count'] = user_profiles['total_count'] - user_profiles['like_count']\n",
    "user_profiles['like_ratio'] = user_profiles['like_count'] / user_profiles['total_count']\n",
    "\n",
    "# Define the percentile thresholds for outliers (manually adjusted)\n",
    "lower_thresh = user_profiles['like_ratio'].quantile(0.05)\n",
    "upper_thresh = user_profiles['like_ratio'].quantile(0.98)\n",
    "\n",
    "# Identify users who are outliers based on the like ratio\n",
    "outliers = user_profiles[(user_profiles['like_ratio'] < lower_thresh) | \n",
    "                         (user_profiles['like_ratio'] > upper_thresh)]\n",
    "\n",
    "print(f'upper:{upper_thresh}  lower:{lower_thresh}')\n",
    "print(f'outliers removed {outliers.shape[0]} out {nnm_train.shape[0]}')\n",
    "print(user_profiles.describe())\n",
    "print(outliers.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our user profiles, we could potentially remove outliers using like count, rejection count, total count, or like ratio.\n",
    "\n",
    "- **total count**\n",
    "Outliers based on total count are just the most and least active users. It might be good to remove the least active ones\n",
    "- **like count**\n",
    "Same problem as total count while also penalizing \"desirable\" users\n",
    "- **rejection count**\n",
    "Same problem as total count while also penalizing \"undesirable\" users\n",
    "- **like ratio**\n",
    "Penalizes the most and least \"desirable\" users, also removes users with low activity (as their ratio is liklier to be high or low)\n",
    "\n",
    "We think removing outliers would degrade the quality of the data, with the possible exception of removing barely active users.\n",
    "\n",
    "Thus we construct our user-user matrix with the full training set, with na values in is_match dropped (as they represent only 1% of the dataset, and cannot be easily represented). We will not normalize as it is not applicable in our case, and represent likes as 20, rejections as 10, and no interaction as 0. The nmf_optimized subroutine has been adjusted for this, and we belive the larger numbers will allow likes to be more easily distingused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_id = max(nnm_train['user_from_id'].max(), nnm_train['user_to_id'].max())\n",
    "interaction_matrix = np.zeros((max_user_id + 1, max_user_id + 1))\n",
    "\n",
    "\n",
    "# Populate the matrix\n",
    "for _, row in nnm_train.iterrows():\n",
    "    value = 20 if row['is_like'] else 10\n",
    "    interaction_matrix[row['user_from_id'], row['user_to_id']] = value\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df = train_test_split(nnm_train, test_size=0.1)\n",
    "\n",
    "# Mask interactions for validation set\n",
    "for _, row in val_df.iterrows():\n",
    "    interaction_matrix[row['user_from_id'], row['user_to_id']] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply our nmf subroutine to get our prediction matrix. We use k = 64 initially but will optimize this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/666\n",
      "10/666\n",
      "20/666\n",
      "30/666\n",
      "40/666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W, H \u001b[39m=\u001b[39m nmf_optimized(interaction_matrix, \u001b[39m64\u001b[39;49m, tol\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m, debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_iter\u001b[39m=\u001b[39;49m\u001b[39m666\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m prediction_matrix \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39mdot(W, H))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(prediction_matrix\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mnmf_optimized\u001b[0;34m(X, n_components, max_iter, tol, debug)\u001b[0m\n\u001b[1;32m     35\u001b[0m H_update \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(W\u001b[39m.\u001b[39mT, V) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mdot(W\u001b[39m.\u001b[39mT, W), H) \u001b[39m+\u001b[39m epsilon)\n\u001b[1;32m     36\u001b[0m H \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m H_update\n\u001b[0;32m---> 37\u001b[0m new_error \u001b[39m=\u001b[39m nmf_loss(V, W, H)\n\u001b[1;32m     39\u001b[0m \u001b[39m# Check for convergence after updating H\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(error \u001b[39m-\u001b[39m new_error) \u001b[39m<\u001b[39m tol:\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mnmf_loss\u001b[0;34m(V, W, H)\u001b[0m\n\u001b[1;32m      7\u001b[0m error \u001b[39m=\u001b[39m V \u001b[39m-\u001b[39m reconstruction\n\u001b[1;32m      9\u001b[0m \u001b[39m# Apply mask and return frobenius norm.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m masked_error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mwhere(mask, error, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(masked_error, \u001b[39m'\u001b[39m\u001b[39mfro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W, H = nmf_optimized(interaction_matrix, 64, tol=1e-4, debug=True, max_iter=666)\n",
    "prediction_matrix = pd.DataFrame(np.dot(W, H))\n",
    "print(prediction_matrix.shape)\n",
    "\n",
    "print(prediction_matrix.to_numpy().flatten().mean())\n",
    "print(prediction_matrix.to_numpy().flatten().std())\n",
    "print(np.quantile(prediction_matrix.to_numpy().flatten(), 0.299))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6020704361211726\n",
      "Thresh: 0.9372662073337882 {'FP': 4004, 'FN': 468, 'TP': 810, 'TN': 2358, 'Accuracy': 0.41465968586387436, 'Precision': 0.1682592438720399, 'Recall': 0.6338028169014085, 'F1 Score': 0.26592252133946165}\n",
      "Thresh: 1.371535794910176 {'FP': 3263, 'FN': 594, 'TP': 684, 'TN': 3099, 'Accuracy': 0.4951570680628272, 'Precision': 0.17329617430960223, 'Recall': 0.5352112676056338, 'F1 Score': 0.26181818181818184}\n",
      "Thresh: 1.805805382486564 {'FP': 2630, 'FN': 690, 'TP': 588, 'TN': 3732, 'Accuracy': 0.5654450261780105, 'Precision': 0.18272218769422002, 'Recall': 0.460093896713615, 'F1 Score': 0.2615658362989324}\n"
     ]
    }
   ],
   "source": [
    "flat_preds = prediction_matrix.to_numpy().flatten()\n",
    "\n",
    "thresh2 = flat_preds.mean() + flat_preds.std() * 2;\n",
    "thresh3 = flat_preds.mean() + flat_preds.std() * 3;\n",
    "thresh4 = flat_preds.mean() + flat_preds.std() * 4;\n",
    "\n",
    "print(np.quantile(flat_preds, 0.99))\n",
    "\n",
    "\n",
    "print(f\"Thresh: {thresh2} {nmf_validate(prediction_matrix=prediction_matrix, validation_set=val_df, threshhold=thresh2)}\")\n",
    "print(f\"Thresh: {thresh3} {nmf_validate(prediction_matrix=prediction_matrix, validation_set=val_df, threshhold=thresh3)}\")\n",
    "print(f\"Thresh: {thresh4} {nmf_validate(prediction_matrix=prediction_matrix, validation_set=val_df, threshhold=thresh4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now that we now everything works, we need to find the optimal k and threshold.\n",
    "\n",
    "Since training takes a very long time, we will create seperate prediction matrices for differing numbers of components, and initially evaluate them using a statistical threshold 3std from the mean. We then loop through different quantile thresholds and statistical thresholds to find the optimal one. Afterwards, we can pick the best k / threshold combination based on its preformance (accuracy, precision, F1, Etc.)\n",
    "\n",
    "Note that we will be saving the prediction matrices as to not have to recompute them later.\n",
    "\n",
    "Additionally, we observe that due to the mostly sparse matrix (most users have no interaction), and the relative rareness of likes (around 15%), our threshold will likley have to be quite high. We will check quantiles ranging from 0.8 to 1 and thresholds up to 4 standard deviations from the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(prediction_matrix:pd.DataFrame, validation_set, metric='F1 Score'):\n",
    "    best_threshold = 0\n",
    "    best_metric_value = 0\n",
    "    metrics_history = []\n",
    "\n",
    "    # Flatten the prediction matrix to apply quantiles\n",
    "    flattened_predictions = prediction_matrix.to_numpy().flatten()\n",
    "\n",
    "    # Generate quantiles starting from 0.8\n",
    "    quantiles = np.linspace(0.8, 1, 100)  # Adjust number of quantiles as needed for time efficiency\n",
    "\n",
    "    for q in quantiles:\n",
    "        threshold = np.quantile(flattened_predictions, q)\n",
    "\n",
    "        # Validate with the current threshold\n",
    "        validation_results = nmf_validate(prediction_matrix, validation_set, threshold)\n",
    "\n",
    "        # Determine the current metric value\n",
    "        current_metric_value = validation_results[metric]\n",
    "        if current_metric_value == 0:\n",
    "            break\n",
    "        # print(f\"Quantile: {q}, Threshold: {threshold}, {metric}: {current_metric_value}\")\n",
    "\n",
    "        # Store the history of metrics for analysis\n",
    "        metrics_history.append((threshold, current_metric_value))\n",
    "\n",
    "        # Update the best threshold if the current metric is better\n",
    "        if current_metric_value > best_metric_value:\n",
    "            best_metric_value = current_metric_value\n",
    "            best_threshold = threshold\n",
    "\n",
    "    # Check thresholds based of standard deviation from mean\n",
    "    for i in range(1, 5):\n",
    "        threshold = flattened_predictions.mean() + flattened_predictions.std() * i\n",
    "\n",
    "        validation_results = nmf_validate(prediction_matrix, validation_set, threshold)\n",
    "\n",
    "        # Determine the current metric value\n",
    "        current_metric_value = validation_results[metric]\n",
    "\n",
    "        # Store the history of metrics for analysis\n",
    "        metrics_history.append((threshold, current_metric_value))\n",
    "\n",
    "        # Update the best threshold if the current metric is better\n",
    "        if current_metric_value > best_metric_value:\n",
    "            best_metric_value = current_metric_value\n",
    "            best_threshold = threshold\n",
    "\n",
    "\n",
    "    print(f\"found optimal threshold {best_threshold} with F1 {best_metric_value}\")\n",
    "    return best_threshold, best_metric_value, metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_validation(prediction_matrix: pd.DataFrame, val_df:pd.DataFrame):\n",
    "    flat_preds = prediction_matrix.to_numpy().flatten()\n",
    "\n",
    "    thresh = flat_preds.mean() + flat_preds.std() * 3\n",
    "\n",
    "    results = nmf_validate(prediction_matrix, val_df, thresh)\n",
    "\n",
    "    print(results)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/666\n",
      "10/666\n",
      "20/666\n",
      "30/666\n",
      "{'FP': 2544, 'FN': 847, 'TP': 412, 'TN': 3837, 'Accuracy': 0.556151832460733, 'Precision': 0.13937753721244925, 'Recall': 0.3272438443208896, 'F1 Score': 0.19549228944246738}\n"
     ]
    }
   ],
   "source": [
    "# prediction matrix with 256 components\n",
    "W, H = nmf_optimized(interaction_matrix, 256, tol=1e-2, debug=True, max_iter=666)\n",
    "prediction_matrix_256 = pd.DataFrame(np.dot(W, H))\n",
    "\n",
    "prediction_matrix_256.to_csv('./resources/256-cache.csv')\n",
    "\n",
    "initial_validation(prediction_matrix_256, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/666\n",
      "10/666\n",
      "20/666\n",
      "30/666\n",
      "40/666\n",
      "50/666\n",
      "60/666\n",
      "70/666\n",
      "80/666\n",
      "90/666\n",
      "100/666\n",
      "110/666\n",
      "120/666\n",
      "130/666\n",
      "140/666\n",
      "150/666\n",
      "{'FP': 1446, 'FN': 1079, 'TP': 199, 'TN': 4916, 'Accuracy': 0.6695026178010471, 'Precision': 0.1209726443768997, 'Recall': 0.15571205007824726, 'F1 Score': 0.13616147793362982}\n"
     ]
    }
   ],
   "source": [
    "# prediction matrix with 512 components\n",
    "W, H = nmf_optimized(interaction_matrix, 512, tol=1e-2, debug=True, max_iter=666)\n",
    "prediction_matrix_512 = pd.DataFrame(np.dot(W, H))\n",
    "\n",
    "prediction_matrix_512.to_csv('./resources/512-cache.csv')\n",
    "\n",
    "initial_validation(prediction_matrix_512, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/666\n",
      "10/666\n",
      "20/666\n",
      "30/666\n",
      "40/666\n",
      "50/666\n",
      "60/666\n",
      "70/666\n",
      "{'FP': 834, 'FN': 1202, 'TP': 76, 'TN': 5528, 'Accuracy': 0.7335078534031414, 'Precision': 0.08351648351648351, 'Recall': 0.0594679186228482, 'F1 Score': 0.06946983546617916}\n"
     ]
    }
   ],
   "source": [
    "# prediction matrix with 1024 components\n",
    "W, H = nmf_optimized(interaction_matrix, 1024, tol=1e-2, debug=True, max_iter=666)\n",
    "prediction_matrix_1024 = pd.DataFrame(np.dot(W, H))\n",
    "\n",
    "prediction_matrix_1024.to_csv('./resources/1024-cache.csv')\n",
    "\n",
    "initial_validation(prediction_matrix_1024, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/666\n",
      "10/666\n",
      "20/666\n",
      "30/666\n",
      "40/666\n",
      "50/666\n",
      "60/666\n",
      "70/666\n",
      "80/666\n",
      "90/666\n",
      "100/666\n",
      "110/666\n",
      "120/666\n",
      "130/666\n",
      "140/666\n",
      "150/666\n",
      "160/666\n",
      "170/666\n",
      "{'FP': 223, 'FN': 1267, 'TP': 11, 'TN': 6139, 'Accuracy': 0.8049738219895288, 'Precision': 0.04700854700854701, 'Recall': 0.008607198748043818, 'F1 Score': 0.01455026455026455}\n"
     ]
    }
   ],
   "source": [
    "# prediction matrix with 2048 components\n",
    "W, H = nmf_optimized(interaction_matrix, 2048, tol=1e-2, debug=True, max_iter=666)\n",
    "prediction_matrix_2048 = pd.DataFrame(np.dot(W, H))\n",
    "\n",
    "prediction_matrix_2048.to_csv('./resources/2048-cache.csv')\n",
    "\n",
    "initial_validation(prediction_matrix_2048, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3717, 3717)\n",
      "(3717, 3717)\n"
     ]
    }
   ],
   "source": [
    "prediction_matrix_256_read = pd.read_csv('./resources/256-cache.csv', delimiter=',', index_col=0)\n",
    "\n",
    "print(prediction_matrix_256.shape)\n",
    "print(prediction_matrix_256_read.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload prediction matrices\n",
    "\n",
    "prediction_matrix_256 = pd.read_csv('./resources/256-cache.csv', delimiter=',', index_col=0)\n",
    "prediction_matrix_512 = pd.read_csv('./resources/512-cache.csv', delimiter=',', index_col=0)\n",
    "prediction_matrix_1024 = pd.read_csv('./resources/1024-cache.csv', delimiter=',', index_col=0)\n",
    "prediction_matrix_2048 = pd.read_csv('./resources/2048-cache.csv', delimiter=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found optimal threshold 0.0006438129481657227 with F1 0.27842914058053503\n",
      "found optimal threshold 7.926719150537305 with F1 0.3966745843230404\n",
      "found optimal threshold 9.871739404216756 with F1 0.4729276473406803\n",
      "found optimal threshold 9.997559264606931 with F1 0.520923520923521\n"
     ]
    }
   ],
   "source": [
    "# Find optimal thresholds\n",
    "\n",
    "stats_256 = find_optimal_threshold(prediction_matrix_256, val_df)\n",
    "stats_512 = find_optimal_threshold(prediction_matrix_512, val_df)\n",
    "stats_1024 = find_optimal_threshold(prediction_matrix_1024, val_df)\n",
    "stats_2048 = find_optimal_threshold(prediction_matrix_2048, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, it seems that the initial validation function was kind of useless, as the threshold worked better for lower component numbers. Going off the optimal thresholds however, we see a consitent improvement in f1 score as we add more components, going past 2048 is unfeasible though.\n",
    "\n",
    "Running nmf with 2048 components took around 1.5 hours on my machine, which is simply too long. Thus, i decided to save all our prediction matrices and will be using 1024 components going forwards. This provides the best balance of performance and runtime in our opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making our final predictions we will try running nmf on a normalized and min-max scaled interaction matrix. The normalized one will use 1024 components, but since it took so long to run, the min-max scaled version will have to use 512 and a lower tolerence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000\n",
      "10/1000\n",
      "20/1000\n",
      "30/1000\n",
      "40/1000\n",
      "50/1000\n",
      "60/1000\n",
      "70/1000\n",
      "80/1000\n",
      "90/1000\n",
      "100/1000\n",
      "110/1000\n",
      "120/1000\n",
      "130/1000\n",
      "140/1000\n",
      "150/1000\n",
      "160/1000\n",
      "170/1000\n",
      "180/1000\n",
      "190/1000\n",
      "200/1000\n",
      "210/1000\n",
      "220/1000\n",
      "230/1000\n",
      "240/1000\n",
      "250/1000\n",
      "260/1000\n",
      "270/1000\n",
      "280/1000\n",
      "290/1000\n",
      "300/1000\n",
      "310/1000\n",
      "320/1000\n",
      "330/1000\n",
      "340/1000\n",
      "350/1000\n",
      "360/1000\n",
      "370/1000\n",
      "380/1000\n",
      "390/1000\n",
      "400/1000\n",
      "410/1000\n",
      "420/1000\n",
      "430/1000\n",
      "440/1000\n",
      "450/1000\n",
      "460/1000\n",
      "470/1000\n",
      "480/1000\n",
      "490/1000\n",
      "500/1000\n",
      "510/1000\n",
      "520/1000\n",
      "530/1000\n",
      "540/1000\n",
      "550/1000\n",
      "560/1000\n",
      "570/1000\n",
      "580/1000\n",
      "590/1000\n",
      "600/1000\n",
      "610/1000\n",
      "620/1000\n",
      "630/1000\n",
      "640/1000\n",
      "650/1000\n",
      "660/1000\n",
      "670/1000\n",
      "680/1000\n",
      "690/1000\n",
      "700/1000\n",
      "710/1000\n",
      "720/1000\n",
      "730/1000\n",
      "740/1000\n",
      "750/1000\n",
      "760/1000\n",
      "770/1000\n",
      "780/1000\n",
      "790/1000\n",
      "800/1000\n",
      "810/1000\n",
      "820/1000\n",
      "830/1000\n",
      "840/1000\n",
      "850/1000\n",
      "860/1000\n",
      "870/1000\n",
      "880/1000\n",
      "890/1000\n",
      "900/1000\n",
      "910/1000\n",
      "920/1000\n",
      "930/1000\n",
      "940/1000\n",
      "950/1000\n",
      "960/1000\n",
      "970/1000\n",
      "980/1000\n",
      "990/1000\n"
     ]
    }
   ],
   "source": [
    "# Trying it out with normalized interaction-matrix\n",
    "interaction_matrix_norm = (interaction_matrix - interaction_matrix.mean()) / interaction_matrix.std()\n",
    "W, H = nmf_optimized(interaction_matrix_norm, 1024, tol=1e-2, debug=True)\n",
    "prediction_matrix_norm = pd.DataFrame(np.dot(W, H))\n",
    "\n",
    "prediction_matrix_norm.to_csv('./resources/norm-cache.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Trying it out with min-max scaling\u001b[39;00m\n\u001b[1;32m      2\u001b[0m interaction_matrix_scaled \u001b[39m=\u001b[39m (interaction_matrix \u001b[39m-\u001b[39m interaction_matrix\u001b[39m.\u001b[39mmin()) \u001b[39m/\u001b[39m (interaction_matrix\u001b[39m.\u001b[39mmax() \u001b[39m-\u001b[39m interaction_matrix\u001b[39m.\u001b[39mmin())\n\u001b[0;32m----> 4\u001b[0m W, H \u001b[39m=\u001b[39m nmf_optimized(interaction_matrix_scaled, \u001b[39m1024\u001b[39;49m, max_iter\u001b[39m=\u001b[39;49m\u001b[39m666\u001b[39;49m, tol\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m, debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m prediction_matrix_scaled \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39mdot(W, H))\n\u001b[1;32m      7\u001b[0m prediction_matrix_scaled\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m./resources-lab-2/scaled-cache.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m, in \u001b[0;36mnmf_optimized\u001b[0;34m(X, n_components, max_iter, tol, debug)\u001b[0m\n\u001b[1;32m     25\u001b[0m W_update \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(V, H\u001b[39m.\u001b[39mT) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mdot(W, H), H\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m epsilon)\n\u001b[1;32m     26\u001b[0m W \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m W_update\n\u001b[0;32m---> 27\u001b[0m new_error \u001b[39m=\u001b[39m nmf_loss(V, W, H)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Check for convergence after updating W\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(error \u001b[39m-\u001b[39m new_error) \u001b[39m<\u001b[39m tol:\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mnmf_loss\u001b[0;34m(V, W, H)\u001b[0m\n\u001b[1;32m      7\u001b[0m error \u001b[39m=\u001b[39m V \u001b[39m-\u001b[39m reconstruction\n\u001b[1;32m      9\u001b[0m \u001b[39m# Apply mask and return frobenius norm.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m masked_error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mwhere(mask, error, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(masked_error, \u001b[39m'\u001b[39m\u001b[39mfro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trying it out with min-max scaling\n",
    "interaction_matrix_scaled = (interaction_matrix - interaction_matrix.min()) / (interaction_matrix.max() - interaction_matrix.min())\n",
    "\n",
    "W, H = nmf_optimized(interaction_matrix_scaled, 1024, max_iter=666, tol=1e-2, debug=True)\n",
    "prediction_matrix_scaled = pd.DataFrame(np.dot(W, H))\n",
    "\n",
    "prediction_matrix_scaled.to_csv('./resources/scaled-cache.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload normalized prediction matrices\n",
    "\n",
    "prediction_matrix_norm = pd.read_csv('./resources/norm-cache.csv', delimiter=',', index_col=0)\n",
    "prediction_matrix_scaled = pd.read_csv('./resources/scaled-cache.csv', delimiter=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found optimal threshold 0.0098431792684458 with F1 0.1846553966189857\n",
      "found optimal threshold 0.0001418035621525 with F1 0.1752298539751217\n"
     ]
    }
   ],
   "source": [
    "# Optimal thresholds for normalized and scaled\n",
    "stats_norm = find_optimal_threshold(prediction_matrix_norm, val_df)\n",
    "stats_scaled = find_optimal_threshold(prediction_matrix_scaled, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(prediction_matrix, test_set, threshold):\n",
    "    \n",
    "    for index, row in test_set.iterrows():\n",
    "        # min handles indexing mismatch\n",
    "        pred_value = prediction_matrix.iloc[min(row['user_from_id'], 3716), row['user_to_id']]\n",
    "        pred = pred_value > threshold\n",
    "        test_set.at[index, 'is_like'] = pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvements from using normalized or min-max scaled matrix. So our best prediction-matrix was the 2048 component one with the optimal threshold found by the function.\n",
    "\n",
    "We will now use it to make our final predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_threshold = stats_2048[0]\n",
    "final_predictions = prediction_matrix_2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_from_id</th>\n",
       "      <th>user_to_id</th>\n",
       "      <th>is_like</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2644</td>\n",
       "      <td>2595</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>567</td>\n",
       "      <td>2412</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2732</td>\n",
       "      <td>3187</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>783</td>\n",
       "      <td>854</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1104</td>\n",
       "      <td>2723</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16198</th>\n",
       "      <td>2197</td>\n",
       "      <td>1449</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>2507</td>\n",
       "      <td>316</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200</th>\n",
       "      <td>511</td>\n",
       "      <td>889</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16201</th>\n",
       "      <td>2148</td>\n",
       "      <td>2947</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16202</th>\n",
       "      <td>1421</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16203 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_from_id  user_to_id is_like is_match\n",
       "0              2644        2595   False        ?\n",
       "1               567        2412   False        ?\n",
       "2              2732        3187   False        ?\n",
       "3               783         854   False        ?\n",
       "4              1104        2723   False        ?\n",
       "...             ...         ...     ...      ...\n",
       "16198          2197        1449   False        ?\n",
       "16199          2507         316   False        ?\n",
       "16200           511         889   False        ?\n",
       "16201          2148        2947   False        ?\n",
       "16202          1421          91   False        ?\n",
       "\n",
       "[16203 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions(final_predictions, nnm_test, final_threshold)\n",
    "\n",
    "nnm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note / thoughts:\n",
    "- Our predictions are quite \"bad\", with a best f1 score of around 0.3. We think this is due to the sparse data, low like ratio, and limitations of nmf (Changing the threshold would either increase FN or FP, big overlap here and a very small difference can cause big changes). You would get fairly bad dating recommendations\n",
    "- Runtime is also extremely long, upwards of an hour to run nmf_optimized. This resulted in a lot of difficulties during the project and stopped us from trying higher component numbers.\n",
    "- I could not find any mistakes (or changes) that would improve either of these (I tried)\n",
    "- F1 scores have lowered across the board after rerunning things today, we think this is due to something going wrong with our caching of the prediction matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_from_id  user_to_id  is_like is_match\n",
      "0              1136        3141    False    False\n",
      "1              2424        3174    False    False\n",
      "2              1300        3590    False    False\n",
      "3               800        2736    False    False\n",
      "4               883         437    False    False\n",
      "...             ...         ...      ...      ...\n",
      "76387          2376        3057    False    False\n",
      "76388          1163         933    False    False\n",
      "76389          2770        3324    False    False\n",
      "76390           879         785    False    False\n",
      "76391           291         470    False    False\n",
      "\n",
      "[76392 rows x 4 columns]\n",
      "       user_from_id  user_to_id is_like is_match\n",
      "0              2644        2595       ?        ?\n",
      "1               567        2412       ?        ?\n",
      "2              2732        3187       ?        ?\n",
      "3               783         854       ?        ?\n",
      "4              1104        2723       ?        ?\n",
      "...             ...         ...     ...      ...\n",
      "16198          2197        1449       ?        ?\n",
      "16199          2507         316       ?        ?\n",
      "16200           511         889       ?        ?\n",
      "16201          2148        2947       ?        ?\n",
      "16202          1421          91       ?        ?\n",
      "\n",
      "[16203 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('resources-lab-2/lab2_train.csv')\n",
    "test_data = pd.read_csv('resources-lab-2/lab2_test.csv')\n",
    "\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's see how it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to format our data so that we can use the method that we have already implemented in WebLab. I have decided to keep the user's ID instead of relying solely on the index to ensure accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def compute_list_tuple_of_ids_part2(data: DataFrame):\n",
    "    likes_dict = {}\n",
    "    user_ids = set()  # Create a set to store unique user IDs\n",
    "\n",
    "    for index, row in data[data['is_like']].iterrows():\n",
    "        user_from_id = row['user_from_id']\n",
    "        user_to_id = row['user_to_id']\n",
    "        \n",
    "        likes_dict.setdefault(user_from_id, set()).add(user_to_id)\n",
    "        user_ids.add(user_from_id)  # Add user_from_id to the set\n",
    "        user_ids.add(user_to_id)  # Add user_to_id to the set\n",
    "    \n",
    "    max_user_id = max(user_ids)  # Determine the maximum user ID\n",
    "    \n",
    "    list_of_like_tuples = [(i, likes_dict.get(i, set())) for i in range(1, max_user_id + 1)]\n",
    "\n",
    "    return list_of_like_tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to have hash functions. I decided to use the implementation from weblab for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "class HashFunction:\n",
    "    \"\"\"\n",
    "    This HashFunction class can be used to create an unique hash given an alpha and beta.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def hashf(self, x, n):\n",
    "        \"\"\"\n",
    "        Returns a hash given integers x and n.\n",
    "        :param x: The value to be hashed\n",
    "        :param n: The number of unique ids of all sets\n",
    "        :return: The hashed value x given alpha and beta\n",
    "        \"\"\"\n",
    "        hash_value = (self.alpha * x + self.beta) % n\n",
    "        return hash_value\n",
    "\n",
    "def create_hash_functions_part2(k, max_alpha_beta):\n",
    "    \"\"\"\n",
    "    Creates a list of k HashFunction instances with random alpha and beta values.\n",
    "    :param k: The number of hash functions to create\n",
    "    :param max_alpha_beta: The maximum value for alpha and beta\n",
    "    :return: A list of HashFunction instances\n",
    "    \"\"\"\n",
    "    hash_functions = []\n",
    "    for _ in range(k):\n",
    "        alpha = random.randint(1, max_alpha_beta)  # Ensure alpha is not 0\n",
    "        beta = random.randint(0, max_alpha_beta)\n",
    "        hash_function = HashFunction(alpha, beta)\n",
    "        hash_functions.append(hash_function)\n",
    "        return hash_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compute the signature matrix. For that, I will use a slightly modified method that I implemented in Weblab. The input is a set of tuples with the ID, and the output will have the first entry in each row as the ID of the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def compute_signature_part2(hashes: List[HashFunction], ids: List[Tuple[int, set]], n: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function calculates the MinHash signature matrix from the list of tuples (id, set()).\n",
    "    The signature vector starts with the corresponding ID.\n",
    "    :param hashes: The list of hash functions of arbitrary length\n",
    "    :param ids: The list of tuples (id, set())\n",
    "    :return: The MinHash signature matrix\n",
    "    \"\"\"\n",
    "    num_rows = len(ids)\n",
    "    num_cols = len(hashes)\n",
    "\n",
    "    signature_matrix = np.full((num_rows, num_cols + 1), sys.maxsize)\n",
    "\n",
    "    for i, (id_val, set_val) in enumerate(ids):\n",
    "        signature_matrix[i, 0] = id_val  # Set the ID as the first entry in each row\n",
    "        for j, hashF in enumerate(hashes):\n",
    "            for element in set_val:\n",
    "                hash_value = hashF.hashf(element, n)\n",
    "                signature_matrix[i, j + 1] = min(signature_matrix[i,j + 1], hash_value)\n",
    "\n",
    "    return signature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find similar users. To achieve this, I am using Jaccard similarity with thresholds instead of top-k. I believe this approach is more flexible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_users_with_simillar_signature_part2(user_id, signature_matrix, treshold):\n",
    "    users_with_same_signature = []\n",
    "    \n",
    "    signature = None\n",
    "\n",
    "    for row in signature_matrix:\n",
    "        if(row[0] == user_id):\n",
    "            signature = row[1:]\n",
    "    if signature is None:\n",
    "        return []\n",
    "        \n",
    "    for row in signature_matrix:\n",
    "        if jacard_similarity_part2(signature, row[1:]) > treshold:\n",
    "            users_with_same_signature.append(row[0])\n",
    "    \n",
    "    return users_with_same_signature\n",
    "\n",
    "def jacard_similarity_part2(v1, v2):\n",
    "    intersection = len(set(v1).intersection(set(v2)))\n",
    "    union = len(set(v1).union(set(v2)))\n",
    "    return intersection / union\n",
    "\n",
    "def find_tuples_with_user_part2(user, lists):\n",
    "    matching_tuples = []\n",
    "    for item in lists:\n",
    "        if item[0] == user:\n",
    "            matching_tuples.append(item[1])\n",
    "    return matching_tuples\n",
    "\n",
    "def users_likes_part2(users: List[int], list_of_tuples) -> set:\n",
    "    res = set()\n",
    "    for user in users:\n",
    "        likes = find_tuples_with_user_part2(user, list_of_tuples)\n",
    "        for item in likes:\n",
    "            res = res.union(item)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def prediction_for_user_part2(user: int, model: np.ndarray, list_of_tuples, threshold: int) -> set:\n",
    "    users = find_users_with_simillar_signature_part2(user, model, threshold)\n",
    "    return users_likes_part2(users, list_of_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Recommender System\n",
    "\n",
    "To evaluate the performance of our recommender system, we need to split our data into a train set and a test set. We will use a split ratio of 95% for the train set and 5% for the test set. Although this split ratio may not be ideal, we have chosen it for performance constraints.\n",
    "\n",
    "As an example, we will test our system using 100 hash functions with alpha beta max of 5 and n (for modulo) of 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7683881064162754, 0.25806451612903225)\n"
     ]
    }
   ],
   "source": [
    "def find_users_likes_part2(user: int, list_of_tuples) -> set:\n",
    "    for item in list_of_tuples:\n",
    "        if item[0] == user:\n",
    "            return item[1]\n",
    "    return set()\n",
    "\n",
    "\n",
    "\n",
    "train_set = train_data.sample(frac=0.95, random_state=42)  # 95% of the data for training\n",
    "test_set = train_data.drop(train_set.index)  # Remaining 5% for testing\n",
    "\n",
    "def test_model(model: np.ndarray, data: DataFrame, threshold: int, list_of_tuples) -> float:\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    wrong_predictions = 0\n",
    "    total_wrong_predictions = 0\n",
    "    counter = 0\n",
    "    for index, row in data.iterrows():\n",
    "        user_from_id = row['user_from_id']\n",
    "        user_to_id = row['user_to_id']\n",
    "        if row['is_like']:\n",
    "            total_predictions += 1\n",
    "            if user_to_id in prediction_for_user_part2(user_from_id, model, list_of_tuples, threshold):\n",
    "                correct_predictions += 1\n",
    "        if row['is_like'] is False:\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                total_wrong_predictions += 1\n",
    "                if user_to_id in prediction_for_user_part2(user_from_id, model, list_of_tuples, threshold):\n",
    "                    wrong_predictions += 1\n",
    "\n",
    "    return correct_predictions / total_predictions, wrong_predictions / total_wrong_predictions\n",
    "\n",
    "list_of_tuples = compute_list_tuple_of_ids_part2(train_set)\n",
    "model = compute_signature_part2(create_hash_functions_part2(100, 5), list_of_tuples, 10)\n",
    "\n",
    "print(test_model(model, test_set, 0.6, list_of_tuples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN\n",
    "The cell below takes a significant amount of time to run (approximately 60 minutes). In this cell, I tested various hyperparameters and manually selected the ones that I deemed most suitable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.6, k: 10, alpha: 2, n: 5, Correctly Positive: 0.8325508607198748, Correctly Negative: 0.3870967741935484\n",
      "Threshold: 0.6, k: 10, alpha: 2, n: 100, Correctly Positive: 0.4757433489827856, Correctly Negative: 0.16129032258064516\n",
      "Threshold: 0.6, k: 10, alpha: 2, n: 500, Correctly Positive: 0.24726134585289514, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.6, k: 10, alpha: 5, n: 5, Correctly Positive: 0.9405320813771518, Correctly Negative: 0.6129032258064516\n",
      "Threshold: 0.6, k: 10, alpha: 5, n: 100, Correctly Positive: 0.6666666666666666, Correctly Negative: 0.22580645161290322\n",
      "Threshold: 0.6, k: 10, alpha: 5, n: 500, Correctly Positive: 0.25508607198748046, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.6, k: 10, alpha: 10, n: 5, Correctly Positive: 0.812206572769953, Correctly Negative: 0.3225806451612903\n",
      "Threshold: 0.6, k: 10, alpha: 10, n: 100, Correctly Positive: 0.672926447574335, Correctly Negative: 0.3225806451612903\n",
      "Threshold: 0.6, k: 10, alpha: 10, n: 500, Correctly Positive: 0.5743348982785602, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.6, k: 50, alpha: 2, n: 5, Correctly Positive: 0.8309859154929577, Correctly Negative: 0.41935483870967744\n",
      "Threshold: 0.6, k: 50, alpha: 2, n: 100, Correctly Positive: 0.5743348982785602, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.6, k: 50, alpha: 2, n: 500, Correctly Positive: 0.3364632237871675, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.6, k: 50, alpha: 5, n: 5, Correctly Positive: 0.8294209702660407, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.6, k: 50, alpha: 5, n: 100, Correctly Positive: 0.594679186228482, Correctly Negative: 0.22580645161290322\n",
      "Threshold: 0.6, k: 50, alpha: 5, n: 500, Correctly Positive: 0.405320813771518, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.6, k: 50, alpha: 10, n: 5, Correctly Positive: 0.9405320813771518, Correctly Negative: 0.6129032258064516\n",
      "Threshold: 0.6, k: 50, alpha: 10, n: 100, Correctly Positive: 0.672926447574335, Correctly Negative: 0.3225806451612903\n",
      "Threshold: 0.6, k: 50, alpha: 10, n: 500, Correctly Positive: 0.3489827856025039, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.6, k: 100, alpha: 2, n: 5, Correctly Positive: 0.8325508607198748, Correctly Negative: 0.3870967741935484\n",
      "Threshold: 0.6, k: 100, alpha: 2, n: 100, Correctly Positive: 0.5743348982785602, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.6, k: 100, alpha: 2, n: 500, Correctly Positive: 0.3489827856025039, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.6, k: 100, alpha: 5, n: 5, Correctly Positive: 0.838810641627543, Correctly Negative: 0.3870967741935484\n",
      "Threshold: 0.6, k: 100, alpha: 5, n: 100, Correctly Positive: 0.4788732394366197, Correctly Negative: 0.16129032258064516\n",
      "Threshold: 0.6, k: 100, alpha: 5, n: 500, Correctly Positive: 0.24726134585289514, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.6, k: 100, alpha: 10, n: 5, Correctly Positive: 0.9405320813771518, Correctly Negative: 0.6129032258064516\n",
      "Threshold: 0.6, k: 100, alpha: 10, n: 100, Correctly Positive: 0.7683881064162754, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.6, k: 100, alpha: 10, n: 500, Correctly Positive: 0.5743348982785602, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.7, k: 10, alpha: 2, n: 5, Correctly Positive: 0.8200312989045383, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.7, k: 10, alpha: 2, n: 100, Correctly Positive: 0.46635367762128327, Correctly Negative: 0.12903225806451613\n",
      "Threshold: 0.7, k: 10, alpha: 2, n: 500, Correctly Positive: 0.25508607198748046, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.7, k: 10, alpha: 5, n: 5, Correctly Positive: 0.8278560250391236, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.7, k: 10, alpha: 5, n: 100, Correctly Positive: 0.4287949921752739, Correctly Negative: 0.1935483870967742\n",
      "Threshold: 0.7, k: 10, alpha: 5, n: 500, Correctly Positive: 0.27230046948356806, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.7, k: 10, alpha: 10, n: 5, Correctly Positive: 0.8200312989045383, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.7, k: 10, alpha: 10, n: 100, Correctly Positive: 0.672926447574335, Correctly Negative: 0.3225806451612903\n",
      "Threshold: 0.7, k: 10, alpha: 10, n: 500, Correctly Positive: 0.49295774647887325, Correctly Negative: 0.1935483870967742\n",
      "Threshold: 0.7, k: 50, alpha: 2, n: 5, Correctly Positive: 0.8309859154929577, Correctly Negative: 0.41935483870967744\n",
      "Threshold: 0.7, k: 50, alpha: 2, n: 100, Correctly Positive: 0.49295774647887325, Correctly Negative: 0.1935483870967742\n",
      "Threshold: 0.7, k: 50, alpha: 2, n: 500, Correctly Positive: 0.25508607198748046, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.7, k: 50, alpha: 5, n: 5, Correctly Positive: 0.8309859154929577, Correctly Negative: 0.41935483870967744\n",
      "Threshold: 0.7, k: 50, alpha: 5, n: 100, Correctly Positive: 0.4272300469483568, Correctly Negative: 0.16129032258064516\n",
      "Threshold: 0.7, k: 50, alpha: 5, n: 500, Correctly Positive: 0.4757433489827856, Correctly Negative: 0.16129032258064516\n",
      "Threshold: 0.7, k: 50, alpha: 10, n: 5, Correctly Positive: 0.8200312989045383, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.7, k: 50, alpha: 10, n: 100, Correctly Positive: 0.6572769953051644, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.7, k: 50, alpha: 10, n: 500, Correctly Positive: 0.3333333333333333, Correctly Negative: 0.12903225806451613\n",
      "Threshold: 0.7, k: 100, alpha: 2, n: 5, Correctly Positive: 0.8325508607198748, Correctly Negative: 0.3870967741935484\n",
      "Threshold: 0.7, k: 100, alpha: 2, n: 100, Correctly Positive: 0.49295774647887325, Correctly Negative: 0.1935483870967742\n",
      "Threshold: 0.7, k: 100, alpha: 2, n: 500, Correctly Positive: 0.3489827856025039, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.7, k: 100, alpha: 5, n: 5, Correctly Positive: 0.8309859154929577, Correctly Negative: 0.41935483870967744\n",
      "Threshold: 0.7, k: 100, alpha: 5, n: 100, Correctly Positive: 0.6572769953051644, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.7, k: 100, alpha: 5, n: 500, Correctly Positive: 0.27230046948356806, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.7, k: 100, alpha: 10, n: 5, Correctly Positive: 0.8309859154929577, Correctly Negative: 0.41935483870967744\n",
      "Threshold: 0.7, k: 100, alpha: 10, n: 100, Correctly Positive: 0.48043818466353677, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.7, k: 100, alpha: 10, n: 500, Correctly Positive: 0.5743348982785602, Correctly Negative: 0.25806451612903225\n",
      "Threshold: 0.8, k: 10, alpha: 2, n: 5, Correctly Positive: 0.8325508607198748, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.8, k: 10, alpha: 2, n: 100, Correctly Positive: 0.4757433489827856, Correctly Negative: 0.16129032258064516\n",
      "Threshold: 0.8, k: 10, alpha: 2, n: 500, Correctly Positive: 0.3489827856025039, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.8, k: 10, alpha: 5, n: 5, Correctly Positive: 0.8200312989045383, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.8, k: 10, alpha: 5, n: 100, Correctly Positive: 0.594679186228482, Correctly Negative: 0.22580645161290322\n",
      "Threshold: 0.8, k: 10, alpha: 5, n: 500, Correctly Positive: 0.31768388106416273, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.8, k: 10, alpha: 10, n: 5, Correctly Positive: 0.8435054773082942, Correctly Negative: 0.41935483870967744\n",
      "Threshold: 0.8, k: 10, alpha: 10, n: 100, Correctly Positive: 0.5915492957746479, Correctly Negative: 0.1935483870967742\n",
      "Threshold: 0.8, k: 10, alpha: 10, n: 500, Correctly Positive: 0.28169014084507044, Correctly Negative: 0.0967741935483871\n",
      "Threshold: 0.8, k: 50, alpha: 2, n: 5, Correctly Positive: 0.8325508607198748, Correctly Negative: 0.3548387096774194\n",
      "Threshold: 0.8, k: 50, alpha: 2, n: 100, Correctly Positive: 0.5805946791862285, Correctly Negative: 0.1935483870967742\n",
      "Threshold: 0.8, k: 50, alpha: 2, n: 500, Correctly Positive: 0.24726134585289514, Correctly Negative: 0.06451612903225806\n",
      "Threshold: 0.8, k: 50, alpha: 5, n: 5, Correctly Positive: 0.8200312989045383, Correctly Negative: 0.2903225806451613\n",
      "Threshold: 0.8, k: 50, alpha: 5, n: 100, Correctly Positive: 0.6666666666666666, Correctly Negative: 0.22580645161290322\n",
      "Threshold: 0.8, k: 50, alpha: 5, n: 500, Correctly Positive: 0.27386541471048514, Correctly Negative: 0.06451612903225806\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.6, 0.7, 0.8]  # List of threshold values to test\n",
    "k_values = [10, 50, 100]  # List of k values to test\n",
    "alpha_values = [2, 5, 10]  # List of alpha values to test\n",
    "n = [5, 100, 500]  # List of n values to test\n",
    "\n",
    "for threshold in thresholds:\n",
    "    for k in k_values:\n",
    "        for alpha in alpha_values:\n",
    "            for n_value in n:\n",
    "                list_of_tuples = compute_list_tuple_of_ids_part2(train_set)\n",
    "                model = compute_signature_part2(create_hash_functions_part2(k, alpha), list_of_tuples, n_value)\n",
    "                accuracy, accuracy2 = test_model(model, test_set, threshold, list_of_tuples)\n",
    "                print(f\"Threshold: {threshold}, k: {k}, alpha: {alpha}, n: {n_value}, Correctly Positive: {accuracy}, not Correctly Negative: {accuracy2}\")\n",
    "\n",
    "\n",
    "#From this threshold = 0.8, k = 50, alpha = 5, n = 5  is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my analysis, I have decided to use the following hyperparameters: threshold = 0.8, k = 50, alpha = 5, and n = 50. The accuracy of correctly marked true values is approximately 80%, while the occurrence of false values marked as true is around 30%, which I consider acceptable. However, I am unsure if the testing methods I used are fully representative. The dataset is sparse, which posed a challenge in building the model. I am particularly concerned about the low value of n = 5, but due to time constraints, I was unable to further improve it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_part2(model, test_set, threshold):\n",
    "    \n",
    "    for index, row in test_set.iterrows():\n",
    "        # min handles indexing mismatch\n",
    "        pred = (row['user_to_id'] in  prediction_for_user_part2(row['user_from_id'], model, list_of_tuples, threshold))\n",
    "        test_set.at[index, 'is_like'] = pred\n",
    "\n",
    "model = compute_signature_part2(create_hash_functions_part2(50, 5), list_of_tuples, 50)\n",
    "dis_test = test_data.copy()\n",
    "\n",
    "make_predictions_part2(model, dis_test, 0.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
